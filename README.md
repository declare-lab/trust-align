# Trust Eval

Welcome to **Trust Eval**! üåü  

A comprehensive tool for evaluating the trustworthiness of inline-cited outputs generated by large language models (LLMs) within the Retrieval-Augmented Generation (RAG) framework. Our suite of metrics measures correctness, citation quality, and groundedness.

This is the official implementation of the metrics introduced in the paper *"Measuring and Enhancing Trustworthiness of LLMs in RAG through Grounded Attributions and Learning to Refuse"* (accepted at ICLR '25).

## Installation üõ†Ô∏è

### Prerequisites

- **OS:** Linux  
- **Python:** Versions 3.10 ‚Äì 3.12 (preferably 3.10.13)  
- **GPU:** Compute capability 7.0 or higher (e.g., V100, T4, RTX20xx, A100, L4, H100)

### Steps

1. **Set up a Python environment**

   ```bash
   conda create -n trust_eval python=3.10.13
   conda activate trust_eval
   ```

2. **Install dependencies**

   ```bash
   pip install trust_eval
   ```

   > Note: that vLLM will be installed with CUDA 12.1. Please ensure your CUDA setup is compatible.

3. **Set up NLTK**

   ```bash
   import nltk
   nltk.download('punkt_tab')
   ```

4. **Download benchmark datasets**
Please download the evaluation dataset from [Huggingface](https://huggingface.co/datasets/declare-lab/Trust-Score/tree/main/Trust-Score) and place the folder as the same level as the prompt folder (see demo for example).

## Quickstart üî•

Evaluate your RAG setup with these main 8 lines.

### Generating Responses

```python
from config import EvaluationConfig, ResponseGeneratorConfig
from evaluator import Evaluator
from logging_config import logger
from response_generator import ResponseGenerator

# Configure the response generator
generator_config = ResponseGeneratorConfig.from_yaml(yaml_path="generator_config.yaml")

# Generate and save responses
generator = ResponseGenerator(generator_config)
generator.generate_responses()
generator.save_responses()
```

### Evaluating Responses

```python
# Configure the evaluator
evaluation_config = EvaluationConfig.from_yaml(yaml_path="eval_config.yaml")

# Compute and save evaluation metrics
evaluator = Evaluator(evaluation_config)
evaluator.compute_metrics()
evaluator.save_results()
```

Please refer to [quickstart](./docs/quickstart/) for the complete guide.

## Contact üì¨

For questions or feedback, reach out to Shang Hong (`simshanghong@gmail.com`).

## Citation üìù

If you use this software in your research, please cite the [Trust-Eval](https://arxiv.org/abs/2409.11242) paper as below.

```bibtex
@misc{song2024measuringenhancingtrustworthinessllms,
      title={Measuring and Enhancing Trustworthiness of LLMs in RAG through Grounded Attributions and Learning to Refuse}, 
      author={Maojia Song and Shang Hong Sim and Rishabh Bhardwaj and Hai Leong Chieu and Navonil Majumder and Soujanya Poria},
      year={2024},
      eprint={2409.11242},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2409.11242}, 
}
```
