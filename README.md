# Trust Eval

Trust Eval is a holistic metric for evaluating trustworthiness of inline cited LLM outputs within the RAG framework. 

## Project Structure

```text
trust-eval/
├── trust-eval/
│   ├── __init__.py
│   ├── config.py
│   ├── llm.py
│   ├── response_generator.py
│   ├── evaluator.py
│   ├── metrics.py
│   ├── utils.py
├── tests/
│   ├── __init__.py
│   ├── test_response_generator.py
│   ├── test_evaluator.py
├── README.md
├── poetry.lock
├── pyproject.toml
```
