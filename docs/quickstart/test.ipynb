{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shanghong/miniconda3/envs/test/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001b[32m 2025-01-24 03:27:51,862 - INFO - trust_eval version 0.1.0 by Shang Hong Sim initialized.\u001b[0m\n",
      "\u001b[32m 2025-01-24 03:27:52,111 - INFO - ResponseGeneratorConfig(yaml_path='generator_config.yaml', model='Qwen/Qwen2.5-0.5B-Instruct', data_type='asqa', prompt_file='prompts/asqa_rejection.json', eval_file='eval_data/custom_data.json', output_path='output/asqa_eval_top100_calibrated.json', quick_test=None, ndoc=5, shot=2, seed=42, no_doc_in_demo=False, fewer_doc_in_demo=False, ndoc_in_demo=None, no_demo=False, rejection=True, openai_api=False, azure=False, lora_path=None, vllm=True, temperature=0.5, top_p=0.95, max_new_tokens=300, max_length=8192, num_samples=1, rejection_flag=\"I apologize, but I couldn't find an answer\", rejection_threshold=85, autoais_model='google/t5_xxl_true_nli_mixture')\u001b[0m\n",
      "\u001b[32m 2025-01-24 03:27:52,597 - INFO - Loading GTR encoder...\u001b[0m\n",
      "\u001b[32m 2025-01-24 03:27:52,602 - INFO - Load pretrained SentenceTransformer: sentence-transformers/gtr-t5-xxl\u001b[0m\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  6.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading docs from pickle file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m 2025-01-24 03:28:15,061 - INFO - gtr embeddings found, loading...\u001b[0m\n",
      "\u001b[32m 2025-01-24 03:28:28,580 - INFO - Running GTR retrieval...\u001b[0m\n",
      "\u001b[32m 2025-01-24 03:28:33,603 - INFO - Loading AutoAIS model...\u001b[0m\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:11<00:00,  2.20s/it]\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "\u001b[32m 2025-01-24 03:28:46,712 - INFO - AutoAIS model loaded successfully.\u001b[0m\n",
      "\u001b[32m 2025-01-24 03:28:51,191 - INFO - Data saved to eval_data/custom_data.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from trust_eval.config import EvaluationConfig, ResponseGeneratorConfig\n",
    "from trust_eval.evaluator import Evaluator\n",
    "from trust_eval.logging_config import logger\n",
    "from trust_eval.response_generator import ResponseGenerator\n",
    "from trust_eval.retrieval import retrieve\n",
    "from trust_eval.data import construct_data\n",
    "\n",
    "# Get configs\n",
    "generator_config = ResponseGeneratorConfig.from_yaml(yaml_path=\"generator_config.yaml\")\n",
    "logger.info(generator_config)\n",
    "\n",
    "# Construct data\n",
    "questions = [\"Where do Asian elephants live?\", \n",
    "         \"Can Orthodox Jewish people eat shellfish?\", \n",
    "         \"How many species of sea urchin are there?\", \n",
    "         \"How heavy is an adult golden jackal?\",\n",
    "         \"Who owns FX network?\",\n",
    "         \"How many letters are used in the French alphabet?\"]\n",
    "answers = [\"Asian elephants live in the forests and grasslands of South and Southeast Asia, including India, Sri Lanka, Thailand, and Indonesia.\",\n",
    "          \"No, Orthodox Jewish people do not eat shellfish because it is not kosher according to Jewish dietary laws.\",\n",
    "          \"There are about 950 species of sea urchins worldwide.\",\n",
    "          \"An adult golden jackal typically weighs between 6 and 14 kilograms (13 to 31 pounds).\",\n",
    "          \"FX network is owned by The Walt Disney Company.\",\n",
    "          \"The French alphabet uses 26 letters, the same as the English alphabet.\"]\n",
    "\n",
    "raw_docs = retrieve(questions, top_k=5)\n",
    "data = construct_data(questions, answers, raw_docs, generator_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m 2025-01-24 03:29:57,843 - INFO - ResponseGeneratorConfig(yaml_path='generator_config.yaml', model='Qwen/Qwen2.5-0.5B-Instruct', data_type='asqa', prompt_file='prompts/asqa_rejection.json', eval_file='eval_data/custom_data.json', output_path='output/asqa_eval_top100_calibrated.json', quick_test=None, ndoc=5, shot=2, seed=42, no_doc_in_demo=False, fewer_doc_in_demo=False, ndoc_in_demo=None, no_demo=False, rejection=True, openai_api=False, azure=False, lora_path=None, vllm=False, temperature=0.5, top_p=0.95, max_new_tokens=300, max_length=8192, num_samples=1, rejection_flag=\"I apologize, but I couldn't find an answer\", rejection_threshold=85, autoais_model='google/t5_xxl_true_nli_mixture')\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "generator_config = ResponseGeneratorConfig.from_yaml(yaml_path=\"generator_config.yaml\")\n",
    "logger.info(generator_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m 2025-01-24 03:29:58,376 - INFO - Loading Qwen/Qwen2.5-0.5B-Instruct in torch.float16...\u001b[0m\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`size` -2GB is not in a valid format. Use an integer for bytes, or a string with an unit (like '5.0GB').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generator\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m generator \u001b[38;5;241m=\u001b[39m \u001b[43mResponseGenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m data \u001b[38;5;241m=\u001b[39m generator\u001b[38;5;241m.\u001b[39mgenerate_responses(data)\n\u001b[1;32m      4\u001b[0m generator\u001b[38;5;241m.\u001b[39msave_responses(output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput/custom_data.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/work/trust_eval/trust_eval/response_generator.py:20\u001b[0m, in \u001b[0;36mResponseGenerator.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config: ResponseGeneratorConfig):\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m config\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm \u001b[38;5;241m=\u001b[39m \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_max_length(config\u001b[38;5;241m.\u001b[39mmodel, config\u001b[38;5;241m.\u001b[39mmax_length)\n\u001b[1;32m     22\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSet the model max length to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/work/trust_eval/trust_eval/llm.py:27\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_vllm(args)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_custom_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/trust_eval/trust_eval/llm.py:49\u001b[0m, in \u001b[0;36mLLM._initialize_custom_model\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_initialize_custom_model\u001b[39m(\u001b[38;5;28mself\u001b[39m, args):\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlora_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlora_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/trust_eval/trust_eval/utils.py:52\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(model_name_or_path, dtype, int8, lora_path, reserve_memory)\u001b[0m\n\u001b[1;32m     50\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse LLM.int8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     51\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 52\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_max_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_in_8bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mint8\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lora_path:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpeft\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PeftModel\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.10/site-packages/transformers/modeling_utils.py:4149\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4144\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m   4145\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis model has some weights that should be kept in higher precision, you need to upgrade \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4146\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`accelerate` to properly deal with them (`pip install --upgrade accelerate`).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4147\u001b[0m     )\n\u001b[1;32m   4148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device_map \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msequential\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 4149\u001b[0m     max_memory \u001b[38;5;241m=\u001b[39m \u001b[43mget_balanced_memory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_zero\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbalanced_low_0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4153\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4154\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdevice_map_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4155\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4156\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4157\u001b[0m     max_memory \u001b[38;5;241m=\u001b[39m get_max_memory(max_memory)\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.10/site-packages/accelerate/utils/modeling.py:932\u001b[0m, in \u001b[0;36mget_balanced_memory\u001b[0;34m(model, max_memory, no_split_module_classes, dtype, special_dtypes, low_zero)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;66;03m# Get default / clean up max_memory\u001b[39;00m\n\u001b[1;32m    931\u001b[0m user_not_set_max_memory \u001b[38;5;241m=\u001b[39m max_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 932\u001b[0m max_memory \u001b[38;5;241m=\u001b[39m \u001b[43mget_max_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_memory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_npu_available():\n\u001b[1;32m    935\u001b[0m     expected_device_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.10/site-packages/accelerate/utils/modeling.py:798\u001b[0m, in \u001b[0;36mget_max_memory\u001b[0;34m(max_memory)\u001b[0m\n\u001b[1;32m    796\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m max_memory:\n\u001b[1;32m    797\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(max_memory[key], \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 798\u001b[0m         max_memory[key] \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_file_size_to_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_memory\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;66;03m# Need to sort the device by type to make sure that we allocate the gpu first.\u001b[39;00m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# As gpu/npu/xpu are represented by int, we need to sort them first.\u001b[39;00m\n\u001b[1;32m    802\u001b[0m gpu_devices \u001b[38;5;241m=\u001b[39m [k \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m max_memory\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(k, \u001b[38;5;28mint\u001b[39m)]\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.10/site-packages/accelerate/utils/modeling.py:144\u001b[0m, in \u001b[0;36mconvert_file_size_to_int\u001b[0;34m(size)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err_msg)\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mem_size \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err_msg)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mem_size\n",
      "\u001b[0;31mValueError\u001b[0m: `size` -2GB is not in a valid format. Use an integer for bytes, or a string with an unit (like '5.0GB')."
     ]
    }
   ],
   "source": [
    "# Generator\n",
    "generator = ResponseGenerator(generator_config)\n",
    "data = generator.generate_responses(data)\n",
    "generator.save_responses(output_path = \"output/custom_data.json\")\n",
    "\n",
    "# Evaluate\n",
    "evaluation_config = EvaluationConfig.from_yaml(yaml_path=\"eval_config.yaml\")\n",
    "logger.info(evaluation_config)\n",
    "evaluator = Evaluator(evaluation_config)\n",
    "evaluator.compute_metrics()\n",
    "evaluator.save_results(output_path = \"results/custom_data.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "hagrid = datasets.load_dataset(\"miracl/hagrid\", split=\"dev\")\n",
    "print(hagrid[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
